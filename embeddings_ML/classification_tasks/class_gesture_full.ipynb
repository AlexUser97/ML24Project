{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DTbvbacyHYow"
      },
      "source": [
        "# Обучение регрессионных задач по проекту ML 2024"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U87VYrzEHYox"
      },
      "source": [
        "## California housing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yB-aVfwJcci8"
      },
      "source": [
        "Установка необходимых библиотек."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IrDZ6j5ucci8",
        "outputId": "72e97650-9138-4228-c34d-af33731d92c9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: catboost in /usr/local/lib/python3.10/dist-packages (1.2.3)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.10/dist-packages (from catboost) (0.20.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from catboost) (3.7.1)\n",
            "Requirement already satisfied: numpy>=1.16.0 in /usr/local/lib/python3.10/dist-packages (from catboost) (1.25.2)\n",
            "Requirement already satisfied: pandas>=0.24 in /usr/local/lib/python3.10/dist-packages (from catboost) (1.5.3)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from catboost) (1.11.4)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.10/dist-packages (from catboost) (5.15.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from catboost) (1.16.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24->catboost) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24->catboost) (2023.4)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (4.49.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (24.0)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (3.1.2)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from plotly->catboost) (8.2.3)\n",
            "Requirement already satisfied: rarfile in /usr/local/lib/python3.10/dist-packages (4.1)\n",
            "Requirement already satisfied: lightgbm in /usr/local/lib/python3.10/dist-packages (4.1.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from lightgbm) (1.25.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from lightgbm) (1.11.4)\n"
          ]
        }
      ],
      "source": [
        "#pip install lightgbm\n",
        "!pip install catboost\n",
        "#pip install xgboost\n",
        "#pip install torch\n",
        "!pip install rarfile\n",
        "# !pip install xgboost\n",
        "!pip install lightgbm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SEh7FC7-HYox"
      },
      "source": [
        "Импорт используемых библиотек."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "STDKFMT_09LM"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.linear_model import SGDRegressor\n",
        "from sklearn import linear_model\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "from scipy.io import arff\n",
        "from os import listdir, getcwd, chdir, mkdir\n",
        "import xgboost\n",
        "import lightgbm as lgb\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.kernel_ridge import KernelRidge\n",
        "import numpy as np\n",
        "from sklearn.utils import resample\n",
        "from scipy import stats\n",
        "from scipy.stats import t\n",
        "from catboost import CatBoostRegressor\n",
        "import math\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "from collections import OrderedDict\n",
        "from typing import Any, Dict, List, Optional, Union\n",
        "try:\n",
        "    import sklearn.tree as sklearn_tree\n",
        "except ImportError:\n",
        "    sklearn_tree = None\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import Tensor\n",
        "try:\n",
        "    from tqdm import tqdm\n",
        "except ImportError:\n",
        "    tqdm = None\n",
        "\n",
        "import lightgbm as lgb\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from xgboost import XGBClassifier\n",
        "from lightgbm import LGBMClassifier\n",
        "from catboost import CatBoostClassifier\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "import xgboost as xgb\n",
        "import lightgbm as lg\n",
        "import rarfile\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YqkMGTHZcci9"
      },
      "source": [
        "Для простоты я написал несколько функций, их импортироуем тут."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "lnv1jNMEHYoy"
      },
      "outputs": [],
      "source": [
        "def boostraping(model, x_test,\n",
        "                y_test, n_iterations = 20):\n",
        "  \"\"\"forms list of rmse scores\n",
        "\n",
        "  Keyword arguments:\n",
        "  model -- fitted model\n",
        "  x_test -- features of test data\n",
        "  y_test -- labels of test data\n",
        "  n_iterations -- number of boostraping procedures (default 20)\n",
        "  \"\"\"\n",
        "  accuracy = []\n",
        "  for i in range(n_iterations):\n",
        "      X_bs, y_bs = resample(x_test, y_test, replace=True)\n",
        "      # make predictions\n",
        "      y_hat = model.predict(X_bs)\n",
        "      # evaluate model\n",
        "      score = accuracy_score(y_bs, y_hat)\n",
        "      accuracy.append(score)\n",
        "  return accuracy\n",
        "\n",
        "def project_train_simple(x_train, x_test, y_train, y_test):\n",
        "  \"\"\"Train project models and generate rmse scores and fitted models\n",
        "\n",
        "  Keyword arguments:\n",
        "  x_train -- features of train data\n",
        "  y_train -- labels of train data\n",
        "  x_test -- features of test data\n",
        "  y_test -- labels of test data\n",
        "  \"\"\"\n",
        "  models = [\n",
        "            CatBoostClassifier(), LogisticRegression(),\n",
        "            RandomForestClassifier(), XGBClassifier(), LGBMClassifier()\n",
        "            ]\n",
        "\n",
        "  test_score = []\n",
        "  train_score = []\n",
        "  model = []\n",
        "  best_parameters = []\n",
        "  for i, k in enumerate(models):\n",
        "    print('Training '+str(k.__class__.__name__))\n",
        "    # Step 1: Fit model\n",
        "    k.fit(x_train, y_train)\n",
        "    model.append(k.__class__.__name__)\n",
        "    # Step 2: Calculate rmse scores\n",
        "    train_score.append(accuracy_score(y_train, k.predict(x_train)))\n",
        "    test_score.append(accuracy_score(y_test, k.predict(x_test)))\n",
        "    best_parameters.append(k)\n",
        "  data = {'model name': model, 'train accuracy': train_score, 'test accuracy': test_score}\n",
        "  df = pd.DataFrame(data)\n",
        "  return df, best_parameters\n",
        "\n",
        "def one_sample_t_test(sample, population_mean, alpha=0.05, tail=\"two\"):\n",
        "    \"\"\"Make t-test and check\n",
        "    H0: mean value of sample == population_mean\n",
        "\n",
        "    Keyword arguments:\n",
        "    sample -- list of means of fetches\n",
        "    population_mean -- mean of distribution\n",
        "    alpha -- the threshold value for rejecting a H0 (default 0.05) 0.01 for 99%\n",
        "    tail -- method of the p-value calculation (default = \"two\", \"left\", \"right\")\n",
        "    \"\"\"\n",
        "    # Step 1: Calculate T-score\n",
        "    sample_mean = np.mean(sample)\n",
        "    sample_std = np.std(sample, ddof=1)\n",
        "    sample_size = len(sample)\n",
        "\n",
        "    t_score = (sample_mean - population_mean) / \\\n",
        "        (sample_std / np.sqrt(sample_size))\n",
        "\n",
        "    # Step 2: Determine degrees of freedom\n",
        "    df = sample_size - 1\n",
        "\n",
        "    # Step 3: Identify the appropriate t-distribution\n",
        "    # No need to explicitly specify degrees of freedom for one-sample t-test in scipy.stats.t\n",
        "\n",
        "    # Step 4: Find the p-value\n",
        "    if tail == \"two\":\n",
        "        p_value = t.sf(np.abs(t_score), df) * 2  # for two-tailed test\n",
        "    elif tail == \"left\":\n",
        "        p_value = t.sf(t_score, df)  # for left-tailed test\n",
        "    elif tail == \"right\":\n",
        "        p_value = t.sf(-t_score, df)  # for right-tailed test\n",
        "    else:\n",
        "        raise ValueError(\n",
        "            \"Invalid tail argument. Use 'two', 'left', or 'right'.\")\n",
        "\n",
        "    # Step 5: Interpret the p-value\n",
        "    print(\"P-value:\", p_value)\n",
        "\n",
        "    if p_value < alpha:\n",
        "        print(\n",
        "            \"Reject the null hypothesis. There is a statistically significant difference.\")\n",
        "    else:\n",
        "        print(\"Fail to reject the null hypothesis. There is no statistically significant difference.\")\n",
        "    return p_value\n",
        "\n",
        "\n",
        "def project_train(x_train, x_test, y_train, y_test):\n",
        "  \"\"\"Train project models on gridsearchCV and generate rmse scores and fitted models\n",
        "\n",
        "  Keyword arguments:\n",
        "  x_train -- features of train data\n",
        "  y_train -- labels of train data\n",
        "  x_test -- features of test data\n",
        "  y_test -- labels of test data\n",
        "  \"\"\"\n",
        "  models = [\n",
        "            CatBoostClassifier(), LogisticRegression(),\n",
        "            RandomForestClassifier(), XGBClassifier(), LGBMClassifier()\n",
        "            ]\n",
        "\n",
        "  grid = [\n",
        "          {'depth': [4, 6, 8],\n",
        "          'learning_rate': [0.01, 0.05, 0.1],\n",
        "          'iterations': [100, 200]},\n",
        "\n",
        "          {'C': [0.1, 1, 10],\n",
        "          'penalty': ['none', 'l2']},\n",
        "\n",
        "          {'n_estimators': [100, 200, 300],\n",
        "          'max_depth': [None, 10, 20]},\n",
        "\n",
        "          {'max_depth': [3, 5, 7],\n",
        "          'learning_rate': [0.01, 0.05, 0.1],\n",
        "          'n_estimators': [100, 200]},\n",
        "\n",
        "          {'max_depth': [3, 5, 7],\n",
        "          'learning_rate': [0.01, 0.05, 0.1],\n",
        "          'n_estimators': [100, 200]}\n",
        "          ]\n",
        "\n",
        "  test_score = []\n",
        "  train_score = []\n",
        "  model = []\n",
        "  best_parameters = []\n",
        "  for i, k in enumerate(models):\n",
        "    print('Training '+str(k.__class__.__name__))\n",
        "    clf = GridSearchCV(k, grid[i])\n",
        "    # Step 1: Fit model on gridsearchCV\n",
        "    best_estimator = clf.fit(x_train, y_train)\n",
        "    best_parameters.append(best_estimator)\n",
        "    model.append(k.__class__.__name__)\n",
        "    # Step 2: Calculate rmse scores\n",
        "    train_score.append(accuracy_score(y_train, best_estimator.predict(x_train)))\n",
        "    test_score.append(accuracy_score(y_test, best_estimator.predict(x_test)))\n",
        "\n",
        "  data = {'model name': model, 'train accuracy': train_score, 'test accuracy': test_score}\n",
        "  df = pd.DataFrame(data)\n",
        "  return df, best_parameters\n",
        "\n",
        "def _check_bins(bins: List[Tensor]) -> None:\n",
        "    if not bins:\n",
        "        raise ValueError('The list of bins must not be empty')\n",
        "    for i, feature_bins in enumerate(bins):\n",
        "        if not isinstance(feature_bins, Tensor):\n",
        "            raise ValueError(\n",
        "                'bins must be a list of PyTorch tensors. '\n",
        "                f'However, for {i=}: {type(bins[i])=}'\n",
        "            )\n",
        "        if feature_bins.ndim != 1:\n",
        "            raise ValueError(\n",
        "                'Each item of the bin list must have exactly one dimension.'\n",
        "                f' However, for {i=}: {bins[i].ndim=}'\n",
        "            )\n",
        "        if len(feature_bins) < 2:\n",
        "            raise ValueError(\n",
        "                'All features must have at least two bin edges.'\n",
        "                f' However, for {i=}: {len(bins[i])=}'\n",
        "            )\n",
        "        if not feature_bins.isfinite().all():\n",
        "            raise ValueError(\n",
        "                'Bin edges must not contain nan/inf/-inf.'\n",
        "                f' However, this is not true for the {i}-th feature'\n",
        "            )\n",
        "        if (feature_bins[:-1] >= feature_bins[1:]).any():\n",
        "            raise ValueError(\n",
        "                'Bin edges must be sorted.'\n",
        "                f' However, the for the {i}-th feature, the bin edges are not sorted'\n",
        "            )\n",
        "        if len(feature_bins) == 2:\n",
        "            warnings.warn(\n",
        "                f'The {i}-th feature has just two bin edges, which means only one bin.'\n",
        "                ' Strictly speaking, using a single bin for the'\n",
        "                ' piecewise-linear encoding should not break anything,'\n",
        "                ' but it is the same as using sklearn.preprocessing.MinMaxScaler'\n",
        "            )\n",
        "\n",
        "\n",
        "def compute_bins(\n",
        "    X: torch.Tensor,\n",
        "    n_bins: int = 48,\n",
        "    *,\n",
        "    tree_kwargs: Optional[Dict[str, Any]] = None,\n",
        "    y: Optional[Tensor] = None,\n",
        "    regression: Optional[bool] = None,\n",
        "    verbose: bool = False,\n",
        ") -> List[Tensor]:\n",
        "    \"\"\"Compute bin edges for `PiecewiseLinearEmbeddings`.\n",
        "\n",
        "    **Usage**\n",
        "\n",
        "    Computing the quantile-based bins (Section 3.2.1 in the paper):\n",
        "\n",
        "    >>> X_train = torch.randn(10000, 2)\n",
        "    >>> bins = compute_bins(X_train)\n",
        "\n",
        "    Computing the tree-based bins (Section 3.2.2 in the paper):\n",
        "\n",
        "    >>> X_train = torch.randn(10000, 2)\n",
        "    >>> y_train = torch.randn(len(X_train))\n",
        "    >>> bins = compute_bins(\n",
        "    ...     X_train,\n",
        "    ...     y=y_train,\n",
        "    ...     regression=True,\n",
        "    ...     tree_kwargs={'min_samples_leaf': 64, 'min_impurity_decrease': 1e-4},\n",
        "    ... )\n",
        "\n",
        "    Args:\n",
        "        X: the training features.\n",
        "        n_bins: the number of bins.\n",
        "        tree_kwargs: keyword arguments for `sklearn.tree.DecisionTreeRegressor`\n",
        "            (if ``regression`` is `True`) or `sklearn.tree.DecisionTreeClassifier`\n",
        "            (if ``regression`` is `False`).\n",
        "            NOTE: requires ``scikit-learn>=1.0,>2`` to be installed.\n",
        "        y: the training labels (must be provided if ``tree`` is not None).\n",
        "        regression: whether the labels are regression labels\n",
        "            (must be provided if ``tree`` is not None).\n",
        "        verbose: if True and ``tree_kwargs`` is not None, than ``tqdm``\n",
        "            (must be installed) will report the progress while fitting trees.\n",
        "    Returns:\n",
        "        A list of bin edges for all features. For one feature:\n",
        "\n",
        "        - the maximum possible number of bin edges is ``n_bins + 1``.\n",
        "        - the minumum possible number of bin edges is ``1``.\n",
        "    \"\"\"\n",
        "    if not isinstance(X, Tensor):\n",
        "        raise ValueError(f'X must be a PyTorch tensor, however: {type(X)=}')\n",
        "    if X.ndim != 2:\n",
        "        raise ValueError(f'X must have exactly two dimensions, however: {X.ndim=}')\n",
        "    if X.shape[0] < 2:\n",
        "        raise ValueError(f'X must have at least two rows, however: {X.shape[0]=}')\n",
        "    if X.shape[1] < 1:\n",
        "        raise ValueError(f'X must have at least one column, however: {X.shape[1]=}')\n",
        "    if not X.isfinite().all():\n",
        "        raise ValueError('X must not contain nan/inf/-inf.')\n",
        "    if (X == X[0]).all(dim=0).any():\n",
        "        raise ValueError(\n",
        "            'All columns of X must have at least two distinct values.'\n",
        "            ' However, X contains columns with just one distinct value.'\n",
        "        )\n",
        "    if n_bins <= 1 or n_bins >= len(X):\n",
        "        raise ValueError(\n",
        "            'n_bins must be more than 1, but less than len(X), however:'\n",
        "            f' {n_bins=}, {len(X)=}'\n",
        "        )\n",
        "\n",
        "    if tree_kwargs is None:\n",
        "        if y is not None or regression is not None or verbose:\n",
        "            raise ValueError(\n",
        "                'If tree_kwargs is None, then y must be None, regression must be None'\n",
        "                ' and verbose must be False'\n",
        "            )\n",
        "\n",
        "        # NOTE[DIFF]\n",
        "        # The original implementation in the official paper repository has an\n",
        "        # unintentional divergence from what is written in the paper.\n",
        "        # This package implements the algorithm described in the paper,\n",
        "        # and it is recommended for future work\n",
        "        # (this may affect the optimal number of bins\n",
        "        #  reported in the official repository).\n",
        "        #\n",
        "        # Additional notes:\n",
        "        # - this is the line where the divergence happens:\n",
        "        #   (the thing is that limiting the number of quantiles by the number of\n",
        "        #   distinct values is NOT the same as removing identical quantiles\n",
        "        #   after computing them)\n",
        "        #   https://github.com/yandex-research/tabular-dl-num-embeddings/blob/c1d9eb63c0685b51d7e1bc081cdce6ffdb8886a8/bin/train4.py#L612C30-L612C30\n",
        "        # - for the tree-based bins, there is NO such divergence;\n",
        "        bins = [\n",
        "            q.unique()\n",
        "            for q in torch.quantile(\n",
        "                X, torch.linspace(0.0, 1.0, n_bins + 1).to(X), dim=0\n",
        "            ).T\n",
        "        ]\n",
        "        _check_bins(bins)\n",
        "        return bins\n",
        "    else:\n",
        "        if sklearn_tree is None:\n",
        "            raise RuntimeError(\n",
        "                'The scikit-learn package is missing.'\n",
        "                ' See README.md for installation instructions'\n",
        "            )\n",
        "        if y is None or regression is None:\n",
        "            raise ValueError(\n",
        "                'If tree_kwargs is not None, then y and regression must not be None'\n",
        "            )\n",
        "        if y.ndim != 1:\n",
        "            raise ValueError(f'y must have exactly one dimension, however: {y.ndim=}')\n",
        "        if len(y) != len(X):\n",
        "            raise ValueError(\n",
        "                f'len(y) must be equal to len(X), however: {len(y)=}, {len(X)=}'\n",
        "            )\n",
        "        if y is None or regression is None:\n",
        "            raise ValueError(\n",
        "                'If tree_kwargs is not None, then y and regression must not be None'\n",
        "            )\n",
        "        if 'max_leaf_nodes' in tree_kwargs:\n",
        "            raise ValueError(\n",
        "                'tree_kwargs must not contain the key \"max_leaf_nodes\"'\n",
        "                ' (it will be set to n_bins automatically).'\n",
        "            )\n",
        "\n",
        "        if verbose:\n",
        "            if tqdm is None:\n",
        "                raise ImportError('If verbose is True, tqdm must be installed')\n",
        "            tqdm_ = tqdm\n",
        "        else:\n",
        "            tqdm_ = lambda x: x  # noqa: E731\n",
        "\n",
        "        if X.device.type != 'cpu' or y.device.type != 'cpu':\n",
        "            warnings.warn(\n",
        "                'Computing tree-based bins involves the conversion of the input PyTorch'\n",
        "                ' tensors to NumPy arrays. The provided PyTorch tensors are not'\n",
        "                ' located on CPU, so the conversion has some overhead.',\n",
        "                UserWarning,\n",
        "            )\n",
        "        X_numpy = X.cpu().numpy()\n",
        "        y_numpy = y.cpu().numpy()\n",
        "        bins = []\n",
        "        for column in tqdm_(X_numpy.T):\n",
        "            feature_bin_edges = [float(column.min()), float(column.max())]\n",
        "            tree = (\n",
        "                (\n",
        "                    sklearn_tree.DecisionTreeRegressor\n",
        "                    if regression\n",
        "                    else sklearn_tree.DecisionTreeClassifier\n",
        "                )(max_leaf_nodes=n_bins, **tree_kwargs)\n",
        "                .fit(column.reshape(-1, 1), y_numpy)\n",
        "                .tree_\n",
        "            )\n",
        "            for node_id in range(tree.node_count):\n",
        "                # The following condition is True only for split nodes. Source:\n",
        "                # https://scikit-learn.org/1.0/auto_examples/tree/plot_unveil_tree_structure.html#tree-structure\n",
        "                if tree.children_left[node_id] != tree.children_right[node_id]:\n",
        "                    feature_bin_edges.append(float(tree.threshold[node_id]))\n",
        "            bins.append(torch.as_tensor(feature_bin_edges).unique())\n",
        "        _check_bins(bins)\n",
        "        return [x.to(device=X.device, dtype=X.dtype) for x in bins]\n",
        "\n",
        "def emb_ple(x, bin):\n",
        "    x = np.array(x)\n",
        "    x_emb = np.zeros(len(bin)-1)\n",
        "    for i in range(len(x_emb)):\n",
        "        if (x < bin[i]) and (i > 0):\n",
        "            x_emb[i] = 0\n",
        "        elif  (x >= bin[i+1]) and (i < len(x_emb)):\n",
        "            x_emb[i] = 1\n",
        "        else:\n",
        "             x_emb[i] = (x - bin[i])/(bin[i+1] - bin[i])\n",
        "    return x_emb\n",
        "\n",
        "\n",
        "def ple(bins, x):\n",
        "    x = np.array(x)\n",
        "    bin = bins[0].numpy() # for torch tensors\n",
        "    #bin = bins[0]\n",
        "    f1 = emb_ple(x[0, 0], bin)\n",
        "    for k in range(len(x)-1):\n",
        "        f2 = emb_ple(x[k+1, 0], bin)\n",
        "        f1 = np.vstack([f1, f2])\n",
        "    x_ple = f1\n",
        "    for i in range(len(x[0])-1):\n",
        "        bin = bins[i+1].numpy()\n",
        "        #bin = bins[i+1]\n",
        "        f1 = emb_ple(x[0, i+1], bin)\n",
        "        for k in range(len(x)-1):\n",
        "            f2 = emb_ple(x[k+1, i+1], bin)\n",
        "            f1 = np.vstack([f1, f2])\n",
        "        x_ple = np.concatenate([x_ple, f1], axis = 1)\n",
        "    return x_ple\n",
        "\n",
        "# x = np.array([[5, 1, 6, 3], [2, 5, 1, 3], [5, 1, 6, 3]])\n",
        "# bin = np.array([[0, 2, 4, 8, 10], [0, 2, 4, 8, 10], [0, 2, 4, 8, 10], [0, 2, 4, 8, 10]])\n",
        "# bins = compute_bins(torch.from_numpy(x_train))\n",
        "# x_emb = ple(bins, x_train)\n",
        "# print(x_emb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1WTWvcyzMsj0"
      },
      "source": [
        "Тут я загружаю данные и разделяю их.  \n",
        "Для некоторых моделей следует использовать шкалирование, а на такие, как catboost шкалирование не влияет. Поэтому перед разделением проведено шкалирование."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i1kx0Ph11rv5",
        "outputId": "550ce85a-d5ad-4d91-b79f-8ecde11d19d9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[           nan  1.0000000e+00            nan ...  5.7159525e-40\n",
            "   1.9208524e-01  5.7159525e-40]\n",
            " [           nan  1.0000000e+00            nan ...  2.8579762e-40\n",
            "   9.6492887e-02  2.8579762e-40]\n",
            " [           nan  1.0000000e+00            nan ...  0.0000000e+00\n",
            "  -0.0000000e+00  0.0000000e+00]\n",
            " ...\n",
            " [           nan  1.0000000e+00            nan ...  0.0000000e+00\n",
            "  -0.0000000e+00  0.0000000e+00]\n",
            " [           nan  1.0000000e+00            nan ...  5.7159525e-40\n",
            "   1.9208524e-01  5.7159525e-40]\n",
            " [           nan  1.0000000e+00            nan ...  5.7159525e-40\n",
            "   1.9208524e-01  5.7159525e-40]]\n"
          ]
        }
      ],
      "source": [
        "# housing = fetch_california_housing()\n",
        "# x = housing.data\n",
        "# y = housing.target\n",
        "# scaler = StandardScaler()\n",
        "# scaler.fit_transform(x)\n",
        "def archive(file):\n",
        "  archive = rarfile.RarFile(file)\n",
        "  archive.extractall()\n",
        "  archive.close()\n",
        "  return archive\n",
        "archive('gesture.rar')\n",
        "x = pd.DataFrame(data=np.concatenate((np.load('gesture/X_num_train.npy'), np.load('gesture/X_num_val.npy'), np.load('gesture/X_num_test.npy')), axis=0))\n",
        "y = pd.Series(data=np.concatenate((np.load('gesture/y_train.npy'), np.load('gesture/y_val.npy'), np.load('gesture/y_test.npy')), axis=0))\n",
        "categorical_features = x.select_dtypes(include=['category', 'object']).columns\n",
        "for feature in categorical_features:\n",
        "  x[feature] = x[feature].astype(str)\n",
        "numeric_categorical_indices = [x.columns.get_loc(col) for col in categorical_features]\n",
        "numeric_features = x.select_dtypes(include=['number'])\n",
        "if not numeric_features.empty:\n",
        "  scaler = StandardScaler()\n",
        "  x[numeric_features.columns] = scaler.fit_transform(numeric_features)\n",
        "\n",
        "label_encoder = LabelEncoder()\n",
        "for feature in categorical_features:\n",
        "  x[feature] = label_encoder.fit_transform(x[feature])\n",
        "if pd.api.types.is_categorical_dtype(pd.Series(y)):\n",
        "  y = y.astype(str)\n",
        "  y = label_encoder.fit_transform(y)\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, random_state=0)\n",
        "#x_train_val, x_test, y_train_val, y_test = train_test_split(x, y)\n",
        "#x_train, x_val, y_train, y_val = train_test_split(x_train_val, y_train_val)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BGiOXBYycci_"
      },
      "source": [
        "### Стандартные модели"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F8POrx_-HYo0"
      },
      "source": [
        "Обучаем модели из коробки"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zu5jOehzHYo1"
      },
      "source": [
        "Обучение на всех моделях кроме LightGBM. Её обучаю отдельно из-за того, что она нестантартно запускается."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "njMU4rY8HYo1"
      },
      "outputs": [],
      "source": [
        "df, models = project_train_simple(x_train, x_test, y_train, y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dKHtXDmHV9Gd"
      },
      "outputs": [],
      "source": [
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uyOdf1a_cci_"
      },
      "source": [
        "Обучение LGBM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w438WmGEHYo1"
      },
      "outputs": [],
      "source": [
        "# train_dataset = lgb.Dataset(x_train, y_train)\n",
        "# test_dataset = lgb.Dataset(x_test, y_test)\n",
        "\n",
        "# booster = lgb.train({\"objective\": \"regression\"},\n",
        "#                     train_set=train_dataset, valid_sets=(test_dataset,),\n",
        "#                     num_boost_round=20)\n",
        "# df.loc[ len(df.index )] = ['LGBM',\n",
        "#                            (accuracy_score(y_train, booster.predict(x_train))),\n",
        "#                            accuracy_score(y_test, booster.predict(x_test))]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A6cBcCPLHYo1"
      },
      "source": [
        "Статистика по моделям из коробки по сравнению с значением метрики в статье."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZX8VVAOxHYo1"
      },
      "outputs": [],
      "source": [
        "p_values = []\n",
        "for i, model in enumerate(models):\n",
        "    rsme = np.array(boostraping(model, x_test, y_test))\n",
        "    print(rsme.mean())\n",
        "    p_value = one_sample_t_test(rsme, 0.683, tail=\"two\")\n",
        "    p_values.append(p_value)\n",
        "# rsme = boostraping(booster, x_test, y_test)\n",
        "# p_value = one_sample_t_test(rsme, 0.683, tail=\"two\")\n",
        "# p_values.append(p_value)\n",
        "df['p_value'] = p_values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7E67LUG0HYo1"
      },
      "outputs": [],
      "source": [
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "8Jm4wz9Sip0U"
      },
      "outputs": [],
      "source": [
        "# @title model name\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "import seaborn as sns\n",
        "df.groupby('model name').size().plot(kind='barh', color=sns.palettes.mpl_palette('Dark2'))\n",
        "plt.gca().spines[['top', 'right',]].set_visible(False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jKd1_8nrM2LX"
      },
      "source": [
        "Заметно, что catboost из коробки дает результат практически равный полученному в статье(больше на 0.013), чем в статье.  \n",
        "При этом хуже всего показал себя RandomForestRegressor - результат более чем вдвое хуже получененого в статье(0.85 против 0.43).  \n",
        "Попробуем улучшить результат результат в следующем разделе. При этом все результаты статистически отличаются от полученного в статье"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "izz4Tk08ccjA"
      },
      "source": [
        "### Модели после GridsearchCV"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mOaH4ZLJccjA"
      },
      "source": [
        "Проведем обучение с использованием уже написанных функций."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4cJsX8LHVCJB"
      },
      "outputs": [],
      "source": [
        "df2, best_params = project_train(x_train, x_test, y_train, y_test)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sJS4Ir6wMhXX"
      },
      "source": [
        "Результат выводится в виде датафрейма для простоты работы с ним."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9-zpt6UJHYo2"
      },
      "outputs": [],
      "source": [
        "df2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hxe2HWfPccjA"
      },
      "source": [
        "Отдельно обучим LGBM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TZw3jISyHYo2"
      },
      "outputs": [],
      "source": [
        "# param_grid = {\n",
        "#     'num_leaves': [31, 127],\n",
        "#     'reg_alpha': [0.1, 0.5],\n",
        "#     'max_depth': [-1, 2, 5],\n",
        "#     'n_estimators': [100, 300],\n",
        "#     'n_jobs': [-1]\n",
        "#     }\n",
        "# lgb_estimator = lgb.LGBMRegressor()\n",
        "# gsearch = GridSearchCV(estimator=lgb_estimator, param_grid=param_grid)\n",
        "# booster = gsearch.fit(X=x_train, y=y_train)\n",
        "# #booster = lgb.train({\"objective\": \"regression\"},\n",
        "# #                    train_set=train_dataset, valid_sets=(test_dataset,),\n",
        "# #                    num_boost_round=20)\n",
        "# df2.loc[ len(df2.index )] = ['LGBM',\n",
        "#                            (mean_squared_error(y_train, booster.predict(x_train), squared = False)),\n",
        "#                            mean_squared_error(y_test, booster.predict(x_test), squared = False)]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2G6AMkh0ccjA"
      },
      "source": [
        "Проверим получилось ли статистически значимо улучшить результат по сравнению по сравненимю с обучением на моделях из коробки."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PqVhrECTHYo2"
      },
      "outputs": [],
      "source": [
        "# best_params.append(booster)\n",
        "#best_params = best_params[0:5]\n",
        "p_values = []\n",
        "for i, model in enumerate(best_params):\n",
        "    rsme = np.array(boostraping(model, x_test, y_test))\n",
        "    print(rsme.mean())\n",
        "    p_value = one_sample_t_test(rsme, df['p_value'][i], tail=\"two\")\n",
        "    p_values.append(p_value)\n",
        "df2['p_value'] = p_values\n",
        "df2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hTxvzjUMccjB"
      },
      "source": [
        "Заметно, что удалось статистически значимо улучшить результаты для XGBRegressor, RandomForestRegressor и LGBM."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m5jJyUSMccjB"
      },
      "source": [
        "### Числовые эмбединги PLE-Q"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Qq-6LFNccjB"
      },
      "source": [
        "Переведем данные с использованием эмбединга."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i1t2WWTWccjB"
      },
      "outputs": [],
      "source": [
        "bins = compute_bins(torch.from_numpy(x_train))#ple-q\n",
        "x_train_emb = ple(bins, x_train)\n",
        "x_test_emb = ple(bins, x_test)\n",
        "'''\n",
        "bins = compute_bins(\n",
        "    torch.from_numpy(x_train),\n",
        "    tree_kwargs={'min_samples_leaf': 64, 'min_impurity_decrease': 1e-4},\n",
        "    y=torch.from_numpy(y_train),\n",
        "    regression=True,\n",
        ")\n",
        "x_train_emb = ple(bins, x_train)\n",
        "x_test_emb = ple(bins, x_test) #ple-t\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VnWsQJ_CGj_m"
      },
      "outputs": [],
      "source": [
        "df4, models_simple_ple_q = project_train_simple(x_train, x_test, y_train, y_test)\n",
        "# train_dataset = lgb.Dataset(x_train, y_train)\n",
        "# test_dataset = lgb.Dataset(x_test, y_test)\n",
        "\n",
        "# booster = lgb.train({\"objective\": \"regression\"},\n",
        "#                     train_set=train_dataset, valid_sets=(test_dataset,),\n",
        "#                     num_boost_round=20)\n",
        "# df4.loc[ len(df4.index )] = ['LGBM',\n",
        "#                            (mean_squared_error(y_train, booster.predict(x_train), squared = False)),\n",
        "#                            mean_squared_error(y_test, booster.predict(x_test), squared = False)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TlyDjW3aN9M-"
      },
      "outputs": [],
      "source": [
        "df4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JRLVPp9_ccjB"
      },
      "source": [
        "Обучим модели"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ChQgmzdsccjB"
      },
      "outputs": [],
      "source": [
        "df3, best_params_ple_q = project_train(x_train_emb, x_test_emb, y_train, y_test)\n",
        "# param_grid = {\n",
        "#     'num_leaves': [31, 127],\n",
        "#     'reg_alpha': [0.1, 0.5],\n",
        "#     'max_depth': [-1, 2, 5],\n",
        "#     'n_estimators': [100, 300],\n",
        "#     'n_jobs': [-1]\n",
        "#     }\n",
        "# lgb_estimator = lgb.LGBMRegressor()\n",
        "# gsearch = GridSearchCV(estimator=lgb_estimator, param_grid=param_grid)\n",
        "# booster = gsearch.fit(X=x_train, y=y_train)\n",
        "# #booster = lgb.train({\"objective\": \"regression\"},\n",
        "# #                    train_set=train_dataset, valid_sets=(test_dataset,),\n",
        "# #                    num_boost_round=20)\n",
        "# df3.loc[ len(df3.index )] = ['LGBM',\n",
        "#                            (mean_squared_error(y_train, booster.predict(x_train), squared = False)),\n",
        "#                            mean_squared_error(y_test, booster.predict(x_test), squared = False)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2u9swRzblR6F"
      },
      "outputs": [],
      "source": [
        "df3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mxC27TZPUNAr"
      },
      "source": [
        "Проверим статистическую значимость различия использования эмбедингов на классическом ML"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "aZxMXkwllV8e"
      },
      "outputs": [],
      "source": [
        "def onetail(grid_models, emb_models, x_test, x_test_emb, y_test, alpha = 0.05):\n",
        "    for i in range(len(grid_models)):\n",
        "        rsme_grid = np.array(boostraping(grid_models[i], x_test, y_test))\n",
        "        rsme_emb = np.array(boostraping(emb_models[i], x_test_emb, y_test))\n",
        "        res = stats.levene(rsme_grid, rsme_emb)\n",
        "        if (res.pvalue > alpha):\n",
        "          t_test = stats.ttest_ind(rsme_grid, rsme_emb)\n",
        "        else:\n",
        "          t_test = stats.ttest_ind(rsme_grid, rsme_emb, equal_var = False)\n",
        "        p_value = t_test.pvalue\n",
        "        # Step 5: Interpret the p-value\n",
        "        print(\"P-value:\", p_value)\n",
        "\n",
        "        if (p_value*0.5 < alpha) and (t_test.statistic<0):\n",
        "          print(\n",
        "            \"Reject the null hypothesis. There is a significant stat improvement for the model \"\n",
        "            +str(grid_models[i].__class__.__name__)+'.')\n",
        "        else:\n",
        "          print(\"Fail to reject the null hypothesis. There is no statistically significant difference for model \"\n",
        "          +str(grid_models[i].__class__.__name__)+'.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vhEzTuxSHYo3"
      },
      "source": [
        "## House_16H\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UNW9mjAtHYo3"
      },
      "source": [
        "Этот датасет пришлось загружать, поэтому он взят с локального устройства.(эта часть проекта сделана в VSC)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RaxqavwfccjI"
      },
      "source": [
        "### Модели из коробки"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mnkn4UvxHYo3"
      },
      "source": [
        "Загрузили, отшкалировали. Теперь обучаем из коробки."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        },
        "id": "ia1QMOkLbl4G",
        "outputId": "6deb4ddb-2c43-492a-bfbf-30c98d96084c"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'best_params' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-10e17fd2740f>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbest_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbest_params\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0monetail\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbest_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_params_ple_q\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_test_emb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'best_params' is not defined"
          ]
        }
      ],
      "source": [
        "best_params = best_params[0:4]\n",
        "onetail(best_params, best_params_ple_q, x_test, x_test_emb, y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xMeArUM8brJl"
      },
      "source": [
        "### Числовые эмбединги PLE-T"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iZqa2turbnAC"
      },
      "outputs": [],
      "source": [
        "bins = compute_bins(\n",
        "    torch.from_numpy(x_train),\n",
        "    tree_kwargs={'min_samples_leaf': 64, 'min_impurity_decrease': 1e-4},\n",
        "    y=torch.from_numpy(y_train),\n",
        "    regression=True,\n",
        ")\n",
        "x_train_emb = ple(bins, x_train)\n",
        "x_test_emb = ple(bins, x_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kY2K6Q3BbvKc"
      },
      "outputs": [],
      "source": [
        "df3, best_params_ple_q = project_train(x_train_emb, x_test_emb, y_train, y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R4i4Wh6jbx5s"
      },
      "outputs": [],
      "source": [
        "onetail(best_params, best_params_ple_q, x_test, x_test_emb, y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rQQYzMNKDRLe"
      },
      "source": [
        "### Pereodic\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "FnMj266QDUqP"
      },
      "outputs": [],
      "source": [
        "from torch.nn.parameter import Parameter\n",
        "\n",
        "def periodic_embedding(X, n_features):\n",
        "  x = torch.from_numpy(np.array(X))\n",
        "  k = 48\n",
        "  sigma = 0.01\n",
        "  weight = Parameter(torch.empty(n_features, k))\n",
        "  x = 2 * math.pi * weight * x[..., None]\n",
        "  x = torch.cat([torch.cos(x), torch.sin(x)], -1)\n",
        "  x = x.detach().numpy()\n",
        "  x_f = x[:, 0, :]\n",
        "  for i in range(x.shape[1]-1):\n",
        "    x_f = np.concatenate([x_f, x[:, i+1, :]], axis = 1)\n",
        "\n",
        "  return x_f"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ytEBOGFKDV2v",
        "outputId": "95c9c0bc-5bb1-4834-e3af-a79a69322d73"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1056"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x_train.shape[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "t5gEZ6gIDYZ3"
      },
      "outputs": [],
      "source": [
        "x = periodic_embedding(x_train, x_train.shape[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bIgsYE4JDaLx",
        "outputId": "c51ed875-c782-4902-9fba-e1c2ce72170a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(7500, 101376)"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2SfQpARgDchi",
        "outputId": "d2429fcb-243a-4eca-c8b3-0e569e1c935f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(7500, 1056)"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x_train.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S_sgpp2WDeW5"
      },
      "outputs": [],
      "source": [
        "x_train_emb = periodic_embedding(x_train, x_train.shape[1])\n",
        "x_test_emb = periodic_embedding(x_test, x_test.shape[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CNhkvz-4Df_4"
      },
      "outputs": [],
      "source": [
        "df3, best_params_ple_q = project_train(x_train_emb, x_test_emb, y_train, y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DzNNwj3ZDih_"
      },
      "outputs": [],
      "source": [
        "df2, best_params = project_train(x_train, x_test, y_train, y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "id": "y-Ka3UfpDkWh",
        "outputId": "e3073ede-f238-405e-f96b-4c6d3555cd38"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'best_params' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-019760371bb1>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0monetail\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbest_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_params_ple_q\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_test_emb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'best_params' is not defined"
          ]
        }
      ],
      "source": [
        "onetail(best_params, best_params_ple_q, x_test, x_test_emb, y_test)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
